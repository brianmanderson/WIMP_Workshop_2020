{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the first script that needs to be run on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you get started:\n",
    "1. Click on \"Runtime\" then \"Change runtime Type\"\n",
    "2. Change hardware accelerator to \"GPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will download the data and unzip it to the server\n",
    "## Do not worry, this will not affect your space on your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, os\n",
    "def unzip(path_to_zip, out_path):\n",
    "  with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
    "      zip_ref.extractall(out_path)\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the module from my github and unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git-lfs clone --recurse-submodules -j8 https://github.com/brianmanderson/WIMP_Workshop_2020\n",
    "os.chdir(os.path.join('.','WIMP_Workshop_2020'))\n",
    "\n",
    "path_to_data = os.path.join('.','Data.zip')\n",
    "out_path = os.path.join('.')\n",
    "print('Unzipping!')\n",
    "unzip(path_to_data,out_path)\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install a few packages that we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tensorflow-gpu==1.15.0'\n",
    "!pip install pydicom==1.2.2\n",
    "!pip install SimpleITK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepBox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we need to import a few things, this includes our generator and visualizing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from Deep_Learning.Base_Deeplearning_Code.Visualizing_Model.Visualing_Model import visualization_model_class\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we need? We need a way to generate larges amounts of training data for our model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_Learning.Shape_Maker import Data_Generator, make_rectangle, make_circle\n",
    "image_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The make_rectangle and make_circle will both return circles and rectangles, and the Data_Generator will randomly create circles or rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(make_rectangle(image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(make_circle(image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our generator essentially continiously creates examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = Data_Generator(image_size=image_size,batch_size=32, num_examples_per_epoch=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y = train_generator.__getitem__(0)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets make our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Activation\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow.python.keras.backend as K\n",
    "from tensorflow.compat.v1 import Graph, Session, ConfigProto, GPUOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will make sure multiple networks don't clog up the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_network():\n",
    "    K.clear_session()\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "    K.set_session(sess)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DeepBox_Network.png](./Deep_Learning/DeepBox_Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep_network()\n",
    "num_kernels = 4\n",
    "kernel_size = (3,3)\n",
    "model = Sequential([\n",
    "    Conv2D(num_kernels, kernel_size, \n",
    "           input_shape=(image_size, image_size, 1), \n",
    "           padding='same',name='Conv_0',activation='sigmoid'),\n",
    "    MaxPool2D((image_size)), # Pool into a 1x1x4 image\n",
    "    Flatten(),\n",
    "    Dense(2,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining loss\n",
    "#### We are specifying that we care about the categorical cross entropy, with a learning rate of 0.1 (very high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=1e-1), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "#### We give the model our generator, and tell it to run for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator,epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "### We will randomly create 500 examples of rectangles and circles and see how well our model does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_accuracy(model, image_size= 64, num_examples=1000):\n",
    "    truth = np.zeros((num_examples,1))\n",
    "    guess = np.zeros((num_examples,1))\n",
    "    index = 0\n",
    "    for _ in range(num_examples//2):\n",
    "        pred = model.predict(make_rectangle(image_size)[None,...,None])\n",
    "        guess[index] = np.argmax(pred)\n",
    "        truth[index] = 1\n",
    "        index += 1\n",
    "    for _ in range(num_examples//2):\n",
    "        pred = model.predict(make_circle(image_size)[None,...,None])\n",
    "        guess[index] = np.argmax(pred)\n",
    "        index += 1\n",
    "    print('Accuracy is {} for {} examples'.format(str((guess==truth).sum()/num_examples),num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "determine_accuracy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see how confident it is in it's predictions, generate a random circle or rectangle and see what the confidence is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectangle = make_rectangle(image_size)[None,...,None]\n",
    "circle = make_circle(image_size)[None,...,None]\n",
    "print('{}% confident'.format(model.predict(rectangle)[...,1][0]*100))\n",
    "print('{}% confident'.format(model.predict(circle)[...,0][0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see what the kernels and activations look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class = visualization_model_class(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Say that we only want to look at Conv_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.define_desired_layers(desired_layer_names=['Conv_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Visualizing_Class.plot_kernels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations\n",
    "### In order to make an activation map we need to provide it with something to predict on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.predict_on_tensor(make_rectangle(image_size)[None,...,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Visualizing_Class.plot_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How big is this model? Super tiny!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def return_sys_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Pre_Processing' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path\n",
    "def return_data_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Data' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pydicom, sys\n",
    "sys.path.append(return_sys_path())\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from Pre_Processing.Distribute_Patients import Separate_files\n",
    "from Pre_Processing.Dicom_RT_and_Images_to_Mask.Image_Array_And_Mask_From_Dicom_RT import Dicom_to_Imagestack\n",
    "from Pre_Processing.Dicom_RT_and_Images_to_Mask.Plot_And_Scroll_Images.Plot_Scroll_Images import plot_Image_Scroll_Bar_Image\n",
    "from Pre_Processing.Make_Single_Images.Make_Single_Images_Class import run_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find where we put our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(return_data_path(),'Data','Whole_Patients')\n",
    "print('We have ' + str(len(os.listdir(data_path))) + ' patients!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring contour fidelity..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that we've set 'get_images_mask' to False, this means we won't be getting any of the image data, just looking at the dicom RT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dicom_Reader = Dicom_to_Imagestack(get_images_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dicom_Reader.down_folder(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What ROI names do we have?\n",
    "\n",
    "#### This will tell us all the unique roi names, hence all_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for roi in Dicom_Reader.all_rois:\n",
    "    print(roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make contour associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have quite a few contour names here.. now, we can either change the ROI names in the RT files, or make an associations file\n",
    "\n",
    "#### The associations file associates a contour name with another one {'Current contour':'Desired name'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations = {'Liver_BMA_Program_4':'Liver',\n",
    "                'bma_liver':'Liver',\n",
    "                'best_liver':'Liver',\n",
    "                'tried_liver':'Liver'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tell the Dicom_Reader that we want to set the associations, get the images and mask for contour 'Liver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dicom_Reader.set_associations(associations)\n",
    "Dicom_Reader.set_get_images_and_mask(True)\n",
    "Dicom_Reader.set_contour_names(['liver'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-write RTs\n",
    "#### This is commented out, because if I run it, then the example above won't show any different contour names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicom_Reader.associations = associations\n",
    "# for RT in Dicom_Reader.all_RTs:\n",
    "#     Dicom_Reader.rewrite_RT(RT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling images and mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll first do this with one patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data_path = os.path.join(data_path,'ABD_LYMPH_036')\n",
    "Dicom_Reader.Make_Contour_From_directory(patient_data_path)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images and mask are saved within the Dicom_Reader class, so we just have to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = Dicom_Reader.ArrayDicom\n",
    "mask = Dicom_Reader.mask # This is the mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images[Images<-200] = -200\n",
    "Images[Images>200] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_Image_Scroll_Bar_Image(Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images[mask==1] += 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "### Checking ROI contour names and making associations\n",
    "\n",
    "### Loading in image and mask from desired contour name\n",
    "\n",
    "### Viewing images and mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate into Train/Test/Validation\n",
    "\n",
    "### This is also important, but I would recommend using the 'Parallel' approach available in https://github.com/brianmanderson/Dicom_Data_to_Numpy_Arrays\n",
    "### For ease, this has already been done for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(data_path, out_path, Dicom_Reader,desc = 'TCIA_Liver_Patients'):\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    print(out_path)\n",
    "    Dicom_Reader.set_description(desc)\n",
    "    iteration = 0\n",
    "    for patient in os.listdir(data_path):\n",
    "        print(patient)\n",
    "        patient_data_path = os.path.join(data_path,patient)\n",
    "        out_file = os.path.join(patient_data_path, desc + '_Iteration_' + str(iteration) + '.txt')\n",
    "        if not os.path.exists(out_file):\n",
    "            Dicom_Reader.Make_Contour_From_directory(patient_data_path)\n",
    "            Dicom_Reader.set_iteration(iteration)\n",
    "            Dicom_Reader.write_images_annotations(out_path)\n",
    "        iteration += 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_path = data_path.replace('Whole_Patients','Niftii_Arrays')\n",
    "#write_data(data_path,output_path, Dicom_Reader)\n",
    "#Separate_files(output_path) # Separate into a Training/Validation/Test set\n",
    "#run_main(output_path,extension=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liver Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def return_sys_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Deep_Learning' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path\n",
    "def return_data_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Data' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(return_sys_path())\n",
    "from Deep_Learning.Base_Deeplearning_Code.Data_Generators.Generators import Data_Generator_Class, os\n",
    "from Deep_Learning.Base_Deeplearning_Code.Keras_Utils.Keras_Utilities import np, dice_coef_3D\n",
    "from Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors import *\n",
    "from Deep_Learning.Base_Deeplearning_Code.Callbacks.Visualizing_Model_Utils import TensorBoardImage\n",
    "from Deep_Learning.Base_Deeplearning_Code.Callbacks.BMA_Callbacks import Add_LR_To_Tensorboard\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "# from Utils import ModelCheckpoint, model_path_maker\n",
    "from Deep_Learning.Base_Deeplearning_Code.Plot_And_Scroll_Images.Plot_Scroll_Images import plot_Image_Scroll_Bar_Image\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow.python.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = return_sys_path()\n",
    "data_path = os.path.join(return_data_path(),'Data','Niftii_Arrays')\n",
    "train_path = [os.path.join(data_path,'Train')]\n",
    "validation_path = os.path.join(data_path,'Validation')\n",
    "test_path = os.path.join(data_path,'Test')\n",
    "model_path = os.path.join(base,'Models')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now need some image processors...\n",
    "\n",
    "#### We will ensure that the images are 256 x 256 (downsampled for speed), normalize them with a mean of 78 and std of 29, add random noise, threshold, and turn into 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "image_processors_train = [Ensure_Image_Proportions(image_size,image_size),Repeat_Channel(repeats=3),\n",
    "                          Normalize_Images(mean_val=78,std_val=29),\n",
    "                          Threshold_Images(lower_bound=-3.55,upper_bound=3.55, post_load=False, final_scale_value=1),\n",
    "                          Annotations_To_Categorical(2)]\n",
    "#Add_Noise_To_Images(variation=np.round(np.arange(start=0, stop=0.3, step=0.1),2)),\n",
    "image_processors_test = [Ensure_Image_Proportions(image_size,image_size),Normalize_Images(mean_val=78,std_val=29),\n",
    "                         Repeat_Channel(repeats=3), \n",
    "                         Threshold_Images(lower_bound=-3.55,upper_bound=3.55, post_load=False,final_scale_value=1),\n",
    "                         Annotations_To_Categorical(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_generator = Data_Generator_Class(by_patient=False,shuffle=True,data_paths=train_path,batch_size=batch_size,\n",
    "                                       image_processors=image_processors_train)\n",
    "validation_generator = Data_Generator_Class(by_patient=True,shuffle=True,data_paths=validation_path,batch_size=30,\n",
    "                                            image_processors=image_processors_test, by_patient_2D=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets visualize one of the examples! With batch_size of 5 and shuffle on, it will be 5 random 2D slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_Image_Scroll_Bar_Image(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright, lets make our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_Learning.Easy_VGG16_UNet.Keras_Fine_Tune_VGG_16_Liver import VGG_16\n",
    "from Deep_Learning.Base_Deeplearning_Code.Visualizing_Model.Visualing_Model import visualization_model_class\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is just a click and play, it builds the VGG16 architecture for you with pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG16_Unet.png](./Deep_Learning/Easy_VGG16_UNet/VGG16_UNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "K.set_session(sess)\n",
    "network = {'Layer_0': {'Encoding': [64, 64], 'Decoding': [64]},\n",
    "           'Layer_1': {'Encoding': [128, 128], 'Decoding': [64]},\n",
    "           'Layer_2': {'Encoding': [256, 256, 256], 'Decoding': [256]},\n",
    "           'Layer_3': {'Encoding': [512, 512, 512], 'Decoding': [256]},\n",
    "           'Layer_4': {'Encoding': [512, 512, 512]}}\n",
    "VGG_model = VGG_16(network=network, activation='relu',filter_size=(3,3))\n",
    "VGG_model.make_model()\n",
    "VGG_model.load_weights()\n",
    "new_model = VGG_model.created_model\n",
    "model_path = os.path.join(return_sys_path(),'Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are some tools for visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class = visualization_model_class(model=new_model, save_images=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the activations of block1_conv1, the activation, and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.define_desired_layers(['block1_conv1','block1_conv1_activation','Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.predict_on_tensor(x[0,...][None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class.plot_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_model.compile(Adam(lr=5e-5),loss='categorical_crossentropy', metrics=['accuracy',dice_coef_3D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing pre-trained layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_until_name(model,name):\n",
    "    set_trainable = False\n",
    "    for layer in model.layers:\n",
    "        if layer.name == name:\n",
    "            set_trainable = True\n",
    "        layer.trainable = set_trainable\n",
    "    return model\n",
    "new_model = freeze_until_name(new_model,'Upsampling0_UNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint and run\n",
    "\n",
    "A checkpoint is a way of assessing the model and determining if we should save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VGG_16_Model'\n",
    "model_path_out = os.path.join(model_path,'VGG_16_frozen')\n",
    "if not os.path.exists(model_path_out):\n",
    "    os.makedirs(model_path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'best-model.hdf5'), monitor='val_dice_coef_3D', verbose=1, save_best_only=True,\n",
    "                              save_weights_only=False, mode='max')\n",
    "# TensorboardImage lets us view the predictions of our model\n",
    "tensorboard = TensorBoardImage(log_dir=model_path_out, num_images=3,update_freq='epoch', \n",
    "                               data_generator=validation_generator, image_frequency=1)\n",
    "callbacks = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets view the model real quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks_v1 import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = TensorBoard(model_path_out)\n",
    "k.set_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit_generator(train_generator,epochs=5, workers=20, max_queue_size=50,\n",
    "                        validation_data=validation_generator,callbacks=callbacks,\n",
    "                        steps_per_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = validation_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = new_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[pred<0.5] = 0\n",
    "pred[pred>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scroll_Image(pred[...,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets make our own architecture\n",
    "\n",
    "### First, lets import some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_Learning.Base_Deeplearning_Code.Models.Keras_Models import my_UNet\n",
    "from Deep_Learning.Base_Deeplearning_Code.Cyclical_Learning_Rate.clr_callback import CyclicLR\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from functools import partial\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our convolution and strided blocks, strided is used for downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {'activation': 'relu'}\n",
    "kernel = (3,3)\n",
    "pool_size = (2,2)\n",
    "#{'channels': x, 'kernel': (3, 3), 'strides': (1, 1),'activation':activation}\n",
    "conv_block = lambda x: {'convolution': {'channels': x, 'kernel': (3, 3),\n",
    "                                        'activation': None, 'strides': (1, 1)}}\n",
    "pooling_downsampling = {'pooling': {'pooling_type': 'Max',\n",
    "                                    'pool_size': (2, 2), 'direction': 'Down'}}\n",
    "pooling_upsampling = {'pooling': {'pool_size': (2, 2), 'direction': 'Up'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our architecture will have 2 main parts in each layer, an 'Encoding' and a 'Decoding' side, 'Encoding' goes down, and 'Decoding' goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoding and Decoding.png](./Deep_Learning/Encoding_and_Decoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now create our layer dictionary, this tells our UNet what to look like\n",
    "\n",
    "### If Pooling is left {} it will perform maxpooling and upsampling with pooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict = {}\n",
    "layers_dict['Layer_0'] = {'Encoding': [],\n",
    "                          'Decoding': [],\n",
    "                          'Pooling':\n",
    "                              {'Encoding': [],\n",
    "                               'Decoding': []\n",
    "                               }}\n",
    "layers_dict['Base'] = []\n",
    "layers_dict['Final_Steps'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict['Layer_0'] = {'Encoding': [conv_block(16),activation,conv_block(16),activation],\n",
    "                          'Decoding': [conv_block(32),activation,conv_block(32),activation],\n",
    "                          'Pooling':\n",
    "                              {'Encoding': [pooling_downsampling],\n",
    "                               'Decoding': [pooling_upsampling]\n",
    "                               }}\n",
    "layers_dict['Base'] = [conv_block(32),activation,conv_block(32),activation]\n",
    "layers_dict['Final_Steps'] = [conv_block(2),{'activation':'softmax'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "K.set_session(sess)\n",
    "new_model = my_UNet(kernel=(3, 3), layers_dict=layers_dict, pool_size=(2, 2), input_size=3, image_size=image_size).created_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name your model and define other things! Send a list of strings and it will make a folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'My_New_Model'\n",
    "model_path_out = os.path.join(model_path,model_name)\n",
    "if not os.path.exists(model_path_out):\n",
    "    os.makedirs(model_path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks_v1 import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = TensorBoard(model_path_out)\n",
    "k.set_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a learning rate and loss metric, also add any metrics you want to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr = 5e-6\n",
    "max_lr = 1e-3\n",
    "new_model.compile(Adam(lr=min_lr),loss='categorical_crossentropy', metrics=['accuracy',dice_coef_3D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a checkpoint to save the model if it has the highest dice, also to add images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will specify that we want to watch the validation dice, and save the one with the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_dice_coef_3D'\n",
    "mode = 'max'\n",
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'best-model.hdf5'), monitor=monitor, verbose=1, save_best_only=True,\n",
    "                             save_weights_only=False, save_freq='epoch', mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, our tensorboard output will add prediction images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorboardImage lets us view the predictions of our model\n",
    "tensorboard = TensorBoardImage(log_dir=model_path_out, num_images=1,update_freq='epoch',\n",
    "                               data_generator=validation_generator, image_frequency=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CyclicLR will allow us to change the learning rate of the model as it runs, and Add_LR_To_Tensorboard will let us view it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_generator)//3\n",
    "step_size_factor = 10\n",
    "\n",
    "cyclic_lrate = CyclicLR(base_lr=min_lr, max_lr=max_lr, step_size=steps_per_epoch * step_size_factor, mode='triangular2')\n",
    "add_lr_to_tensorboard = Add_LR_To_Tensorboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [cyclic_lrate, add_lr_to_tensorboard, tensorboard, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit_generator(train_generator,epochs=10, workers=10, max_queue_size=200, validation_data=validation_generator,\n",
    "                       callbacks=callbacks, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
