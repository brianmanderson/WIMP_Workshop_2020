{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the first script that needs to be run on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you get started:\n",
    "1. Click on \"Runtime\" then \"Change runtime Type\"\n",
    "2. Change hardware accelerator to \"GPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to install some things on the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import zipfile, os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class download_data_from_folder(object):\n",
    "    def __init__(self,path):\n",
    "        path_id = path[path.find('id=')+3:]\n",
    "        self.file_list = self.get_files_in_location(path_id)\n",
    "        self.unwrap_data(self.file_list)\n",
    "    def get_files_in_location(self,folder_id):\n",
    "        file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(folder_id)}).GetList()\n",
    "        return file_list\n",
    "    def unwrap_data(self,file_list,directory='.'):\n",
    "        for i, file in enumerate(file_list):\n",
    "            print(str((i + 1) / len(file_list) * 100) + '% done copying')\n",
    "            if file['mimeType'].find('folder') != -1:\n",
    "                if not os.path.exists(os.path.join(directory, file['title'])):\n",
    "                    os.makedirs(os.path.join(directory, file['title']))\n",
    "                print('Copying folder ' + os.path.join(directory, file['title']))\n",
    "                self.unwrap_data(self.get_files_in_location(file['id']), os.path.join(directory, file['title']))\n",
    "            else:\n",
    "                if not os.path.exists(os.path.join(directory, file['title'])):\n",
    "                    downloaded = drive.CreateFile({'id': file['id']})\n",
    "                    downloaded.GetContentFile(os.path.join(directory, file['title']))\n",
    "        return None\n",
    "def unzip(path_to_zip, out_path):\n",
    "  with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
    "      zip_ref.extractall(out_path)\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, since we're logged into the server, we need to log in to our google account so it can download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will download the data and unzip it to the server\n",
    "## Do not worry, this will not affect your space on your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_path = 'https://drive.google.com/open?id=1fY6B4Q7L2viZmr8cAWl0wCBUeaN0UO1E'\n",
    "download_data_from_folder(data_path)\n",
    "path_to_data = os.path.join('.','Data.zip')\n",
    "out_path = os.path.join('.')\n",
    "print('Unzipping!')\n",
    "unzip(path_to_data,out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recurse-submodules -j8 https://github.com/brianmanderson/Imaging_Physics_Workshop_1_28_20.git\n",
    "os.chdir(os.path.join('.','Imaging_Physics_Workshop_1_28_20'))\n",
    "!pip install 'tensorflow-gpu==1.15.0'\n",
    "!pip install pydicom\n",
    "!pip install SimpleITK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepBox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we need to import a few things, this includes our generator and visualizing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from Base_Deeplearning_Code.Visualizing_Model.Visualing_Model import visualization_model_class\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we need? We need a way to generate larges amounts of training data for our model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Shape_Maker import Data_Generator, make_rectangle, make_circle\n",
    "image_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The make_rectangle and make_circle will both return circles and rectangles, and the Data_Generator will randomly create circles or rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23de5511898>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(make_rectangle(image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23de55b4550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(make_circle(image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our generator essentially continiously creates examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = Data_Generator(image_size=image_size,batch_size=32, num_examples_per_epoch=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64, 64, 1)\n",
      "(32, 2)\n"
     ]
    }
   ],
   "source": [
    "x,y = train_generator.__getitem__(0)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets make our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Activation\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow import Graph, Session, ConfigProto, GPUOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will make sure multiple networks don't clog up the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_network():\n",
    "    K.clear_session()\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "    K.set_session(sess)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DeepBox_Network.png](./DeepBox_Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0129 11:22:34.718690  1236 deprecation_wrapper.py:119] From C:\\Users\\bmanderson\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0129 11:22:34.719722  1236 deprecation_wrapper.py:119] From C:\\Users\\bmanderson\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0129 11:22:34.731730  1236 deprecation_wrapper.py:119] From C:\\Users\\bmanderson\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0129 11:22:34.739744  1236 deprecation_wrapper.py:119] From C:\\Users\\bmanderson\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0129 11:22:34.741750  1236 deprecation_wrapper.py:119] From C:\\Users\\bmanderson\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0129 11:22:34.752805  1236 deprecation_wrapper.py:119] From C:\\Users\\bmanderson\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_network()\n",
    "num_kernels = 4\n",
    "kernel_size = (3,3)\n",
    "model = Sequential([\n",
    "    Conv2D(num_kernels, kernel_size, \n",
    "           input_shape=(image_size, image_size, 1), \n",
    "           padding='same',name='Conv_0',activation='sigmoid'),\n",
    "    MaxPool2D((image_size)), # Pool into a 1x1x4 image\n",
    "    Flatten(),\n",
    "    Dense(2,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining loss\n",
    "#### We are specifying that we care about the categorical cross entropy, with a learning rate of 0.1 (very high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0129 11:22:34.784692  1236 deprecation_wrapper.py:119] From C:\\Users\\bmanderson\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(Adam(lr=1e-1), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "#### We give the model our generator, and tell it to run for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0129 11:22:34.858917  1236 deprecation.py:323] From c:\\users\\bmanderson\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "150/150 [==============================] - 2s 12ms/step - loss: 0.1825 - acc: 0.9454\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 2s 13ms/step - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 2s 12ms/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 2s 13ms/step - loss: 7.3924e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23de55e1cf8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator,epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "### We will randomly create 500 examples of rectangles and circles and see how well our model does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_accuracy(model, image_size= 64, num_examples=1000):\n",
    "    truth = np.zeros((num_examples,1))\n",
    "    guess = np.zeros((num_examples,1))\n",
    "    index = 0\n",
    "    for _ in range(num_examples//2):\n",
    "        pred = model.predict(make_rectangle(image_size)[None,...,None])\n",
    "        guess[index] = np.argmax(pred)\n",
    "        truth[index] = 1\n",
    "        index += 1\n",
    "    for _ in range(num_examples//2):\n",
    "        pred = model.predict(make_circle(image_size)[None,...,None])\n",
    "        guess[index] = np.argmax(pred)\n",
    "        index += 1\n",
    "    print('Accuracy is {} for {} examples'.format(str((guess==truth).sum()/num_examples),num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1.0 for 1000 examples\n"
     ]
    }
   ],
   "source": [
    "determine_accuracy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see how confident it is in it's predictions, generate a random circle or rectangle and see what the confidence is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.93651509284973% confident\n",
      "99.97038245201111% confident\n"
     ]
    }
   ],
   "source": [
    "rectangle = make_rectangle(image_size)[None,...,None]\n",
    "circle = make_circle(image_size)[None,...,None]\n",
    "print('{}% confident'.format(model.predict(rectangle)[...,1][0]*100))\n",
    "print('{}% confident'.format(model.predict(circle)[...,0][0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see what the kernels and activations look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv_0\n",
      "max_pooling2d_1\n",
      "flatten_1\n",
      "dense_1\n"
     ]
    }
   ],
   "source": [
    "Visualizing_Class = visualization_model_class(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Say that we only want to look at Conv_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.define_desired_layers(desired_layer_names=['Conv_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv_0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Visualizing_Class.plot_kernels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations\n",
    "### In order to make an activation map we need to provide it with something to predict on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.predict_on_tensor(make_rectangle(image_size)[None,...,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conv_0']\n",
      "Conv_0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 92.16x92.16 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Visualizing_Class.plot_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How big is this model? Super tiny!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_0 (Conv2D)              (None, 64, 64, 4)         40        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 4)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 50\n",
      "Trainable params: 50\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def return_sys_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Pre_Processing' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pydicom, sys\n",
    "sys.path.append(return_sys_path())\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from Pre_Processing.Distribute_Patients import Separate_files\n",
    "from Pre_Processing.Dicom_RT_and_Images_to_Mask.Image_Array_And_Mask_From_Dicom_RT import Dicom_to_Imagestack\n",
    "from Pre_Processing.Dicom_RT_and_Images_to_Mask.Plot_And_Scroll_Images.Plot_Scroll_Images import plot_Image_Scroll_Bar_Image\n",
    "from Pre_Processing.Make_Single_Images.Make_Single_Images_Class import run_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find where we put our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 20 patients!\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(return_sys_path(),'Data','Whole_Patients')\n",
    "print('We have ' + str(len(os.listdir(data_path))) + ' patients!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring contour fidelity..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that we've set 'get_images_mask' to False, this means we won't be getting any of the image data, just looking at the dicom RT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dicom_Reader = Dicom_to_Imagestack(get_images_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dicom_Reader.down_folder(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What ROI names do we have?\n",
    "\n",
    "#### This will tell us all the unique roi names, hence all_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bma_liver\n",
      "Liver_BMA_Program_4\n",
      "tried_liver\n",
      "best_liver\n",
      "Liver\n"
     ]
    }
   ],
   "source": [
    "for roi in Dicom_Reader.all_rois:\n",
    "    print(roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make contour associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have quite a few contour names here.. now, we can either change the ROI names in the RT files, or make an associations file\n",
    "\n",
    "#### The associations file associates a contour name with another one {'Current contour':'Desired name'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations = {'Liver_BMA_Program_4':'Liver',\n",
    "                'bma_liver':'Liver',\n",
    "                'best_liver':'Liver',\n",
    "                'tried_liver':'Liver'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tell the Dicom_Reader that we want to set the associations, get the images and mask for contour 'Liver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dicom_Reader.set_associations(associations)\n",
    "Dicom_Reader.set_get_images_and_mask(True)\n",
    "Dicom_Reader.set_contour_names(['liver'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-write RTs\n",
    "#### This is commented out, because if I run it, then the example above won't show any different contour names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicom_Reader.associations = associations\n",
    "# for RT in Dicom_Reader.all_RTs:\n",
    "#     Dicom_Reader.rewrite_RT(RT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling images and mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll first do this with one patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "patient_data_path = os.path.join(data_path,'ABD_LYMPH_036')\n",
    "Dicom_Reader.Make_Contour_From_directory(patient_data_path)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images and mask are saved within the Dicom_Reader class, so we just have to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = Dicom_Reader.ArrayDicom\n",
    "mask = Dicom_Reader.mask # This is the mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images[Images<-200] = -200\n",
    "Images[Images>200] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef8912163d24c4a9d93a20f822b4d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Z', max=160), Text(value='2D', description='view'), Outpâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_Image_Scroll_Bar_Image(Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images[mask==1] += 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "### Checking ROI contour names and making associations\n",
    "\n",
    "### Loading in image and mask from desired contour name\n",
    "\n",
    "### Viewing images and mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate into Train/Test/Validation\n",
    "\n",
    "### This is also important, but I would recommend using the 'Parallel' approach available in https://github.com/brianmanderson/Dicom_Data_to_Numpy_Arrays\n",
    "### For ease, this has already been done for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(data_path, out_path, Dicom_Reader,desc = 'TCIA_Liver_Patients'):\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    print(out_path)\n",
    "    Dicom_Reader.set_description(desc)\n",
    "    iteration = 0\n",
    "    for patient in os.listdir(data_path):\n",
    "        print(patient)\n",
    "        patient_data_path = os.path.join(data_path,patient)\n",
    "        out_file = os.path.join(patient_data_path, desc + '_Iteration_' + str(iteration) + '.txt')\n",
    "        if not os.path.exists(out_file):\n",
    "            Dicom_Reader.Make_Contour_From_directory(patient_data_path)\n",
    "            Dicom_Reader.set_iteration(iteration)\n",
    "            Dicom_Reader.write_images_annotations(out_path)\n",
    "        iteration += 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_path = data_path.replace('Whole_Patients','Niftii_Arrays')\n",
    "#write_data(data_path,output_path, Dicom_Reader)\n",
    "#Separate_files(output_path) # Separate into a Training/Validation/Test set\n",
    "#run_main(output_path,extension=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liver Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def return_sys_path():\n",
    "    path = '.'\n",
    "    for _ in range(5):\n",
    "        if 'Deep_Learning' in os.listdir(path):\n",
    "            break\n",
    "        else:\n",
    "            path = os.path.join(path,'..')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0217 17:43:22.398360 17780 deprecation.py:506] From c:\\users\\bmanderson\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(return_sys_path())\n",
    "from Deep_Learning.Base_Deeplearning_Code.Data_Generators.Generators import Data_Generator_Class, os\n",
    "from Deep_Learning.Base_Deeplearning_Code.Keras_Utils.Keras_Utilities import np, dice_coef_3D\n",
    "from Deep_Learning.Base_Deeplearning_Code.Data_Generators.Image_Processors import *\n",
    "from Deep_Learning.Base_Deeplearning_Code.Callbacks.Visualizing_Model_Utils import TensorBoardImage\n",
    "# from Utils import ModelCheckpoint, model_path_maker\n",
    "from Deep_Learning.Base_Deeplearning_Code.Plot_And_Scroll_Images.Plot_Scroll_Images import plot_Image_Scroll_Bar_Image\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = return_sys_path()\n",
    "data_path = os.path.join(base,'Data','Niftii_Arrays')\n",
    "train_path = [os.path.join(data_path,'Train')]\n",
    "validation_path = os.path.join(data_path,'Validation')\n",
    "test_path = os.path.join(data_path,'Test')\n",
    "model_path = os.path.join(base,'Models')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now need some image processors...\n",
    "\n",
    "#### We will ensure that the images are 256 x 256 (downsampled for speed), normalize them with a mean of 78 and std of 29, add random noise, threshold, and turn into 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "image_processors_train = [Ensure_Image_Proportions(image_size,image_size),Repeat_Channel(repeats=3),\n",
    "                          Normalize_Images(mean_val=78,std_val=29),\n",
    "                          Threshold_Images(lower_bound=-3.55,upper_bound=3.55, post_load=False),\n",
    "                          Annotations_To_Categorical(2)]\n",
    "#Add_Noise_To_Images(variation=np.round(np.arange(start=0, stop=0.3, step=0.1),2)),\n",
    "image_processors_test = [Ensure_Image_Proportions(image_size,image_size),Normalize_Images(mean_val=78,std_val=29),\n",
    "                         Repeat_Channel(repeats=3), Threshold_Images(lower_bound=-3.55,upper_bound=3.55),\n",
    "                         Annotations_To_Categorical(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_generator = Data_Generator_Class(by_patient=False,shuffle=True,data_paths=train_path,batch_size=batch_size,\n",
    "                                       image_processors=image_processors_train)\n",
    "validation_generator = Data_Generator_Class(by_patient=True,shuffle=True,data_paths=validation_path,batch_size=30,\n",
    "                                            image_processors=image_processors_test, by_patient_2D=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets visualize one of the examples! With batch_size of 5 and shuffle on, it will be 5 random 2D slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713e7c14ad49405eb07d1b881bf1bba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Z', max=4), Text(value='2D', description='view'), Outputâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_Image_Scroll_Bar_Image(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright, lets make our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_Learning.Easy_VGG16_UNet.Keras_Fine_Tune_VGG_16_Liver import VGG_16\n",
    "from Deep_Learning.Base_Deeplearning_Code.Visualizing_Model.Visualing_Model import visualization_model_class\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is just a click and play, it builds the VGG16 architecture for you with pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG16_Unet.png](./Deep_Learning/Easy_VGG16_UNet/VGG16_UNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "K.set_session(sess)\n",
    "network = {'Layer_0': {'Encoding': [64, 64], 'Decoding': [64]},\n",
    "           'Layer_1': {'Encoding': [128, 128], 'Decoding': [64]},\n",
    "           'Layer_2': {'Encoding': [256, 256, 256], 'Decoding': [256]},\n",
    "           'Layer_3': {'Encoding': [512, 512, 512], 'Decoding': [256]},\n",
    "           'Layer_4': {'Encoding': [512, 512, 512]}}\n",
    "VGG_model = VGG_16(network=network, activation='relu',filter_size=(3,3))\n",
    "VGG_model.make_model()\n",
    "VGG_model.load_weights()\n",
    "new_model = VGG_model.created_model\n",
    "model_path = os.path.join(return_sys_path(),'Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are some tools for visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class = visualization_model_class(model=new_model, save_images=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the activations of block1_conv1, the activation, and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.define_desired_layers(['block1_conv1','block1_conv1_activation','Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing_Class.predict_on_tensor(x[0,...][None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Visualizing_Class.plot_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_model.compile(Adam(lr=5e-5),loss='categorical_crossentropy', metrics=['accuracy',dice_coef_3D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing pre-trained layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_until_name(model,name):\n",
    "    set_trainable = False\n",
    "    for layer in model.layers:\n",
    "        if layer.name == name:\n",
    "            set_trainable = True\n",
    "        layer.trainable = set_trainable\n",
    "    return model\n",
    "new_model = freeze_until_name(new_model,'Upsampling0_UNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint and run\n",
    "\n",
    "A checkpoint is a way of assessing the model and determining if we should save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VGG_16_Model'\n",
    "other_aspects = [model_name,'Upsampling0_UNet_Unfrozen'] # Just a list of defining things\n",
    "model_path_out = model_path_maker(model_path,other_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'best-model.hdf5'), monitor='val_dice_coef_3D', verbose=1, save_best_only=True,\n",
    "                              save_weights_only=False, period=5, mode='max')\n",
    "# TensorboardImage lets us view the predictions of our model\n",
    "tensorboard = TensorBoardImage(log_dir=model_path_out, batch_size=1, num_images=3,update_freq='epoch', \n",
    "                               data_generator=validation_generator)\n",
    "callbacks = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"../Models\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit_generator(train_generator,epochs=1, workers=20, max_queue_size=50, validation_data=validation_generator,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = validation_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = new_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[pred<0.5] = 0\n",
    "pred[pred>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scroll_Image(pred[...,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets make our own architecture\n",
    "\n",
    "### First, lets import some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_Learning.Base_Deeplearning_Code.Models.Keras_3D_Models import my_3D_UNet\n",
    "from Deep_Learning.Base_Deeplearning_Code.Cyclical_Learning_Rate.clr_callback import CyclicLR\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from functools import partial\n",
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our convolution and strided blocks, strided is used for downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'relu'\n",
    "kernel = (3,3)\n",
    "pool_size = (2,2)\n",
    "#{'channels': x, 'kernel': (3, 3), 'strides': (1, 1),'activation':activation}\n",
    "conv_block = lambda x: {'channels': x}\n",
    "strided_block = lambda x: {'channels': x, 'strides': (2, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our architecture will have 2 main parts in each layer, an 'Encoding' and a 'Decoding' side, 'Encoding' goes down, and 'Decoding' goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoding and Decoding.png](./Deep_Learning/Encoding_and_Decoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now create our layer dictionary, this tells our UNet what to look like\n",
    "\n",
    "### If Pooling is left {} it will perform maxpooling and upsampling with pooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dict = {}\n",
    "filters = 16\n",
    "layers_dict['Layer_0'] = {'Encoding':[conv_block(filters),conv_block(filters)],\n",
    "                          'Decoding':[conv_block(filters),conv_block(filters)],\n",
    "                          'Pooling':{}}\n",
    "filters = 32\n",
    "layers_dict['Base'] = {'Encoding':[conv_block(filters), conv_block(filters),conv_block(filters)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Layer_0': {'Encoding': [{'channels': 16}, {'channels': 16}],\n",
       "  'Decoding': [{'channels': 16}, {'channels': 16}],\n",
       "  'Pooling': {}},\n",
       " 'Base': {'Encoding': [{'channels': 32}, {'channels': 32}, {'channels': 32}]}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer_0\n",
      "Base\n",
      "Layer_0\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "K.set_session(sess)\n",
    "new_model = my_3D_UNet(kernel=kernel,layers_dict=layers_dict, pool_size=pool_size,activation=activation,is_2D=True,\n",
    "                       input_size=3, image_size=image_size).created_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a learning rate and loss metric, also add any metrics you want to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr = 5e-6\n",
    "max_lr = 8e-4\n",
    "new_model.compile(Adam(lr=min_lr),loss='categorical_crossentropy', metrics=['accuracy',dice_coef_3D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name your model and define other things! Send a list of strings and it will make a folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path_maker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a598a27ce503>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m other_aspects = [model_name,'{}_Layers'.format(3),'{}_Conv_Blocks'.format(2),\n\u001b[0;32m      3\u001b[0m                  '{}_Filters'.format(16),'{}_MinLR_{}_MaxLR'.format(min_lr,max_lr)] # Just a list of defining things\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_path_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_path_maker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mother_aspects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_path_maker' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = 'My_New_Model'\n",
    "other_aspects = [model_name,'{}_Layers'.format(3),'{}_Conv_Blocks'.format(2),\n",
    "                 '{}_Filters'.format(16),'{}_MinLR_{}_MaxLR'.format(min_lr,max_lr)] # Just a list of defining things\n",
    "model_path_out = model_path_maker(model_path,other_aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few more parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_generator)//3\n",
    "step_size_factor = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a checkpoint to save the model if it has the highest dice, also to add images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will specify that we want to watch the validation dice, and save the one with the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_dice_coef_3D'\n",
    "mode = 'max'\n",
    "checkpoint = ModelCheckpoint(os.path.join(model_path_out,'best-model.hdf5'), monitor=monitor, verbose=1, save_best_only=True,\n",
    "                             save_weights_only=False, period=5, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, our tensorboard output will add prediction images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0217 17:45:50.838107 17780 Visualizing_Model_Utils.py:520] `batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
     ]
    }
   ],
   "source": [
    "# TensorboardImage lets us view the predictions of our model\n",
    "tensorboard = TensorBoardImage(log_dir=os.path.join('.','Models'), batch_size=1, num_images=1,update_freq='epoch', \n",
    "                               data_generator=validation_generator, image_frequency=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CyclicLR will allow us to change the learning rate of the model as it runs, and Add_LR_To_Tensorboard will let us view it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclic_lrate = CyclicLR(base_lr=min_lr, max_lr=max_lr, step_size=steps_per_epoch * step_size_factor, mode='triangular2')\n",
    "add_lr_to_tensorboard = Add_LR_To_Tensorboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [cyclic_lrate, add_lr_to_tensorboard, tensorboard, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0217 17:46:03.330332 17780 deprecation.py:323] From c:\\users\\bmanderson\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/279 [============================>.] - ETA: 0s - loss: 0.6448 - acc: 0.6834 - dice_coef_3D: 0.1135Epoch 1/10\n",
      "279/279 [==============================] - 124s 444ms/step - loss: 0.6445 - acc: 0.6844 - dice_coef_3D: 0.1133 - val_loss: 0.5456 - val_acc: 0.9282 - val_dice_coef_3D: 0.1011\n",
      "Epoch 2/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.4494 - acc: 0.9247 - dice_coef_3D: 0.1299Epoch 1/10\n",
      "279/279 [==============================] - 91s 327ms/step - loss: 0.4490 - acc: 0.9249 - dice_coef_3D: 0.1297 - val_loss: 0.3606 - val_acc: 0.9016 - val_dice_coef_3D: 0.1957\n",
      "Epoch 3/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9347 - dice_coef_3D: 0.1843Epoch 1/10\n",
      "279/279 [==============================] - 82s 295ms/step - loss: 0.2384 - acc: 0.9347 - dice_coef_3D: 0.1845 - val_loss: 0.1918 - val_acc: 0.9103 - val_dice_coef_3D: 0.2922\n",
      "Epoch 4/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9383 - dice_coef_3D: 0.2456Epoch 1/10\n",
      "279/279 [==============================] - 77s 277ms/step - loss: 0.1460 - acc: 0.9384 - dice_coef_3D: 0.2452 - val_loss: 0.1757 - val_acc: 0.8943 - val_dice_coef_3D: 0.3434\n",
      "Epoch 5/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9365 - dice_coef_3D: 0.2935Epoch 1/10\n",
      "279/279 [==============================] - 76s 273ms/step - loss: 0.1266 - acc: 0.9365 - dice_coef_3D: 0.2937 - val_loss: 0.1456 - val_acc: 0.9151 - val_dice_coef_3D: 0.3360\n",
      "Epoch 6/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9377 - dice_coef_3D: 0.3323Epoch 1/10\n",
      "279/279 [==============================] - 78s 278ms/step - loss: 0.1166 - acc: 0.9377 - dice_coef_3D: 0.3322 - val_loss: 0.1640 - val_acc: 0.9068 - val_dice_coef_3D: 0.3472\n",
      "Epoch 7/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9437 - dice_coef_3D: 0.3549Epoch 1/10\n",
      "279/279 [==============================] - 76s 274ms/step - loss: 0.1070 - acc: 0.9438 - dice_coef_3D: 0.3547 - val_loss: 0.1158 - val_acc: 0.9426 - val_dice_coef_3D: 0.3208\n",
      "Epoch 8/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9450 - dice_coef_3D: 0.3933Epoch 1/10\n",
      "279/279 [==============================] - 77s 276ms/step - loss: 0.1057 - acc: 0.9450 - dice_coef_3D: 0.3933 - val_loss: 0.1196 - val_acc: 0.9457 - val_dice_coef_3D: 0.3035\n",
      "Epoch 9/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9503 - dice_coef_3D: 0.4195Epoch 1/10\n",
      "279/279 [==============================] - 76s 274ms/step - loss: 0.1018 - acc: 0.9503 - dice_coef_3D: 0.4193 - val_loss: 0.1372 - val_acc: 0.9303 - val_dice_coef_3D: 0.4522\n",
      "Epoch 10/10\n",
      "278/279 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9580 - dice_coef_3D: 0.4352Epoch 1/10\n",
      "279/279 [==============================] - 77s 278ms/step - loss: 0.0934 - acc: 0.9581 - dice_coef_3D: 0.4352 - val_loss: 0.0851 - val_acc: 0.9682 - val_dice_coef_3D: 0.3873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21389e2d5f8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit_generator(train_generator,epochs=10, workers=50, max_queue_size=200, validation_data=validation_generator,\n",
    "                       callbacks=callbacks, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {\"./Models\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
